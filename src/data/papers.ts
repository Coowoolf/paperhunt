export interface Paper {
    id: string;
    title: string;
    authors: string;
    venue: string;
    year: number;
    arxivId?: string;
    link?: string;
    tags: string[];
    abstract: string;
    citations?: number;
    highlight?: boolean;
    era?: string;
    tier?: string; // 0: Classics, 1: Theory, 2: Ear, 3: Brain, 4: Voice, 5: Support, 6: Global
    mustRead?: boolean;
    oneLiner?: string; // PM-friendly insight summary
}

export const papers: Paper[] = [
    // ==========================================
    // Tier 0: Classics (Hall of Fame)
    // ==========================================
    { id: "turing", title: "Computing Machinery and Intelligence", authors: "Alan Turing", venue: "Mind", year: 1950, link: "https://redirect.cs.umbc.edu/courses/471/papers/turing.pdf", tags: ["foundational", "philosophy", "landmark"], abstract: "The philosophical foundation of AI. Proposed the Turing Test - defining intelligence as the ability to imitate human conversation indistinguishably.", citations: 50000, highlight: true, era: "Genesis", tier: "0", mustRead: true, oneLiner: "If a machine can converse indistinguishably from a human, it thinks" },
    { id: "eliza", title: "ELIZAâ€”A Computer Program for Natural Language Communication", authors: "Joseph Weizenbaum", venue: "CACM", year: 1966, link: "https://dl.acm.org/doi/10.1145/365153.365168", tags: ["chatbot", "pattern-matching", "landmark"], abstract: "First chatbot using pattern matching. Demonstrated the 'Eliza Effect' where users attribute intelligence to simple scripts.", citations: 8500, highlight: true, era: "Genesis", tier: "0", mustRead: true, oneLiner: "Users project intelligence onto simple pattern-matching scripts" },
    { id: "gus", title: "GUS, A Frame-Driven Dialog System", authors: "Bobrow et al.", venue: "Artificial Intelligence", year: 1977, tags: ["foundational", "frame-based", "slot-filling"], abstract: "Introduced frame-based dialogue with slot filling, the ancestor of all task-oriented dialogue systems.", citations: 1200, era: "Genesis", tier: "0" },
    { id: "shrdlu", title: "Understanding Natural Language", authors: "Terry Winograd", venue: "Academic Press", year: 1972, tags: ["foundational", "micro-world", "symbolic"], abstract: "SHRDLU: A system that understood language in a 'blocks world', demonstrating deep semantic understanding in a limited domain.", citations: 4500, era: "Genesis", tier: "0" },
    { id: "perceptrons", title: "Perceptrons", authors: "Minsky & Papert", venue: "MIT Press", year: 1969, tags: ["foundational", "neural-networks"], abstract: "Showed limitations of single-layer perceptrons, shifting funding away from connectionism for a decade (AI Winter).", citations: 12000, era: "Genesis", tier: "0" },
    { id: "deepspeech", title: "Deep Speech: Scaling up end-to-end speech recognition", authors: "Hannun et al.", venue: "arXiv", year: 2014, arxivId: "1412.5567", tags: ["ASR", "deep-learning", "end-to-end"], abstract: "First successful end-to-end deep learning approach to speech recognition at scale.", citations: 5500, era: "Deep Learning", tier: "0", highlight: true },

    // ==========================================
    // Tier 1: Theory (Linguistics & Interaction)
    // ==========================================
    { id: "turn-taking", title: "A Simplest Systematics for Turn-Taking in Conversation", authors: "Sacks, Schegloff, Jefferson", venue: "Language", year: 1974, link: "https://www.jstor.org/stable/412243", tags: ["turn-taking", "sociology", "foundational"], abstract: "Seminal sociology paper defining Transition Relevance Places (TRPs), essential for modern endpointing algorithms.", citations: 25000, highlight: true, era: "Genesis", tier: "1", mustRead: true, oneLiner: "Human turn-taking operates at ~200ms biological limit" },
    { id: "grice", title: "Logic and Conversation", authors: "Paul Grice", venue: "Speech Acts", year: 1975, tags: ["linguistics", "pragmatics"], abstract: "Introduced the Cooperative Principle and Gricean Maxims (Quantity, Quality, Relation, Manner).", citations: 30000, era: "Genesis", tier: "1" },
    { id: "clark-brennan", title: "Grounding in Communication", authors: "Clark & Brennan", venue: "APA", year: 1991, tags: ["psychology", "grounding"], abstract: "Analyzed how common ground is established and maintained in conversation through feedback and repair.", citations: 15000, era: "Statistical", tier: "1" },
    { id: "media-equation", title: "The Media Equation", authors: "Reeves & Nass", venue: "CSLI", year: 1996, tags: ["HCI", "psychology", "CASA"], abstract: "Proved humans subconsciously apply social rules to computers (CASA paradigm).", citations: 8000, era: "Statistical", tier: "1", mustRead: true, oneLiner: "Humans treat computers as social actors unconsciously" },
    { id: "dgslm", title: "Generative Spoken Dialogue Language Modeling", authors: "Meta AI", venue: "arXiv", year: 2022, arxivId: "2203.16502", tags: ["textless", "speech-tokens", "dialogue"], abstract: "First textless spoken dialogue model using speech-to-unit tokens without text intermediates.", citations: 280, era: "Generative", tier: "1", mustRead: true, oneLiner: "Dialogue without the text bottleneck proves speech-native models are possible" },

    // ==========================================
    // Tier 2: Ear (Audio Understanding)
    // ==========================================
    { id: "hmm-tutorial", title: "A Tutorial on HMMs and Applications in Speech Recognition", authors: "Lawrence Rabiner", venue: "IEEE", year: 1989, tags: ["HMM", "ASR", "statistical"], abstract: "The standard reference for Statistical ASR using Hidden Markov Models.", citations: 35000, era: "Statistical", tier: "2" },
    { id: "wavenet", title: "WaveNet: A Generative Model for Raw Audio", authors: "DeepMind", venue: "arXiv", year: 2016, arxivId: "1609.03499", tags: ["audio-generation", "deep-learning", "landmark"], abstract: "Raw audio generation using dilated causal convolutions. Revolutionized TTS quality.", citations: 8000, era: "Deep Learning", tier: "2", mustRead: true },
    { id: "las", title: "Listen, Attend and Spell", authors: "Chan et al.", venue: "ICASSP", year: 2016, arxivId: "1508.01211", tags: ["ASR", "seq2seq", "attention"], abstract: "End-to-end ASR using a pyramidal BLSTM listener and an attention-based speller.", citations: 4000, era: "Deep Learning", tier: "2" },
    { id: "jasper", title: "Jasper: An End-to-End Convolutional Neural Acoustic Model", authors: "Li et al.", venue: "Interspeech", year: 2019, arxivId: "1904.03288", tags: ["ASR", "CNN"], abstract: "Deep convolutional architecture for ASR, showing CNNs can match RNN performance.", citations: 1200, era: "Deep Learning", tier: "2" },
    { id: "conformer", title: "Conformer: Convolution-augmented Transformer for Speech Recognition", authors: "Gulati et al.", venue: "Interspeech", year: 2020, arxivId: "2005.08100", tags: ["ASR", "transformer", "CNN"], abstract: "Combines CNNs and Transformers to capture both local and global dependencies. State-of-the-art encoder.", citations: 3000, era: "Deep Learning", tier: "2", mustRead: true },
    { id: "wav2vec2", title: "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations", authors: "Baevski et al.", venue: "NeurIPS", year: 2020, arxivId: "2006.11477", tags: ["self-supervised", "ASR", "Meta"], abstract: "Masked prediction of quantized speech representations. The BERT of speech.", citations: 6000, era: "Generative", tier: "2", mustRead: true },
    { id: "whisper", title: "Robust Speech Recognition via Large-Scale Weak Supervision", authors: "OpenAI", venue: "ICML", year: 2023, arxivId: "2212.04356", tags: ["ASR", "foundation-model", "OpenAI", "landmark"], abstract: "Trained on 680k hours. Robust ASR that works 'out of the box' across accents and noise.", citations: 2100, highlight: true, era: "Generative", tier: "2", mustRead: true },
    { id: "salmonn", title: "SALMONN: Towards Generic Hearing Abilities for LLMs", authors: "Tang et al.", venue: "ICLR", year: 2024, arxivId: "2310.13289", tags: ["audio-LM", "hearing"], abstract: "Integrates speech and audio encoders with LLMs for generic hearing.", citations: 145, era: "Generative", tier: "2" },
    { id: "qwen2-audio", title: "Qwen2-Audio: Advancing General Audio Perception", authors: "Alibaba", venue: "arXiv", year: 2024, arxivId: "2407.10759", tags: ["audio-LM", "multimodal"], abstract: "Complex audio scene understanding and instruction following.", citations: 89, era: "Generative", tier: "2" },
    { id: "hubert", title: "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction", authors: "Hsu et al.", venue: "arXiv", year: 2021, arxivId: "2106.07447", tags: ["self-supervised", "Meta", "representation"], abstract: "Self-supervised speech representation learning using cluster prediction.", citations: 3200, era: "Generative", tier: "2" },

    // ==========================================
    // Tier 3: Brain (Intelligence & Dialogue)
    // ==========================================
    { id: "seq2seq", title: "Sequence to Sequence Learning with Neural Networks", authors: "Sutskever et al.", venue: "NeurIPS", year: 2014, arxivId: "1409.3215", tags: ["seq2seq", "landmark"], abstract: "The foundation of modern neural translation and dialogue.", citations: 25000, highlight: true, era: "Deep Learning", tier: "3" },
    { id: "attention", title: "Attention Is All You Need", authors: "Vaswani et al.", venue: "NeurIPS", year: 2017, arxivId: "1706.03762", tags: ["transformer", "landmark"], abstract: "Transformer architecture. The basis for BERT, GPT, and all modern LLMs.", citations: 95000, highlight: true, era: "Deep Learning", tier: "3", mustRead: true },
    { id: "bert", title: "BERT: Pre-training of Deep Bidirectional Transformers", authors: "Devlin et al.", venue: "NAACL", year: 2019, arxivId: "1810.04805", tags: ["transformer", "pre-training"], abstract: "Bidirectional pre-training for language understanding.", citations: 80000, era: "Deep Learning", tier: "3" },
    { id: "gpt3", title: "Language Models are Few-Shot Learners", authors: "Brown et al.", venue: "NeurIPS", year: 2020, arxivId: "2005.14165", tags: ["LLM", "few-shot", "OpenAI"], abstract: "GPT-3. Showed that scale enables few-shot learning capabilities.", citations: 15000, era: "Generative", tier: "3", mustRead: true },
    { id: "meena", title: "Recipes for Building an Open-Domain Chatbot", authors: "Google", venue: "arXiv", year: 2020, arxivId: "2001.09977", tags: ["chatbot", "SSA", "Google"], abstract: "Meena chatbot. Proposed the SSA (Sensibleness & Specificity) metric.", citations: 1500, era: "Generative", tier: "3" },
    { id: "lamda", title: "LaMDA: Language Models for Dialog Applications", authors: "Thoppilan et al.", venue: "arXiv", year: 2022, arxivId: "2201.08239", tags: ["chatbot", "safety", "Google"], abstract: "Focus on safety, groundedness, and quality scores for dialogue.", citations: 2000, era: "Generative", tier: "3" },
    { id: "instructgpt", title: "Training Language Models to Follow Instructions", authors: "OpenAI", venue: "NeurIPS", year: 2022, arxivId: "2203.02155", tags: ["RLHF", "alignment", "landmark"], abstract: "RLHF to align models with human intent. The method behind ChatGPT.", citations: 8500, highlight: true, era: "Generative", tier: "3", mustRead: true },
    { id: "llama", title: "LLaMA: Open and Efficient Foundation Language Models", authors: "Touvron et al. (Meta)", venue: "arXiv", year: 2023, arxivId: "2302.13971", tags: ["LLM", "open-source", "Meta"], abstract: "High-performance open models trained on public data. Sparked the open LLM wave.", citations: 5000, era: "Generative", tier: "3" },
    { id: "generative-agents", title: "Generative Agents", authors: "Park et al.", venue: "UIST", year: 2023, arxivId: "2304.03442", tags: ["agents", "simulation"], abstract: "Interactive simulacra of human behavior in Smallville.", citations: 2500, era: "Generative", tier: "3" },
    { id: "gpt-4o", title: "GPT-4o System Card", authors: "OpenAI", venue: "OpenAI", year: 2024, link: "https://openai.com/index/gpt-4o-system-card/", tags: ["multimodal", "omni", "landmark"], abstract: "Native multimodal model with real-time audio and vision.", citations: 890, highlight: true, era: "Generative", tier: "3", mustRead: true },
    { id: "moshi", title: "Moshi: A Full-Duplex Speech-to-Speech Foundation Model", authors: "Kyutai", venue: "arXiv", year: 2024, arxivId: "2410.00037", tags: ["full-duplex", "speech-to-speech", "landmark"], abstract: "Open full-duplex model using Mimi codec.", citations: 156, highlight: true, era: "Generative", tier: "3", mustRead: true },
    { id: "minio-mni", title: "Mini-Omni: Language Models Can Hear, Speak, and Think", authors: "Xie et al.", venue: "arXiv", year: 2024, arxivId: "2408.16725", tags: ["speech-to-speech", "efficient"], abstract: "Streaming speech-to-speech with text instruction.", citations: 45, era: "Generative", tier: "3" },
    { id: "lslm", title: "Language Model Can Listen While Speaking", authors: "Ma et al.", venue: "AAAI", year: 2025, arxivId: "2408.02622", tags: ["full-duplex", "barge-in"], abstract: "Listening-while-speaking architecture for barge-in handling.", citations: 45, era: "Generative", tier: "3", mustRead: true, oneLiner: "The key to natural conversation: listen while you speak" },
    { id: "llama-omni", title: "LLaMA-Omni: Seamless Speech Interaction with Large Language Models", authors: "Fang et al.", venue: "arXiv", year: 2024, arxivId: "2409.06666", tags: ["speech-to-speech", "llama", "low-latency"], abstract: "End-to-end speech interaction built on Llama-3 achieving 226ms latency.", citations: 180, era: "Generative", tier: "3", mustRead: true, oneLiner: "Llama goes voice-native with sub-250ms end-to-end latency" },

    // ==========================================
    // Tier 4: Voice (Audio Generation)
    // ==========================================
    { id: "tacotron", title: "Tacotron: Towards End-to-End Speech Synthesis", authors: "Wang et al.", venue: "Interspeech", year: 2017, arxivId: "1703.10135", tags: ["TTS", "end-to-end"], abstract: "First popular end-to-end TTS generating mel spectrograms from text.", citations: 5000, era: "Deep Learning", tier: "4" },
    { id: "encodec", title: "High Fidelity Neural Audio Compression", authors: "Defense et al. (Meta)", venue: "arXiv", year: 2022, arxivId: "2210.13438", tags: ["codec", "neural-audio", "Meta"], abstract: "Efficient neural audio codec. Used in MusicLM, AudioLM, and Moshi (Mimi variant).", citations: 600, era: "Generative", tier: "4", mustRead: true },
    { id: "soundstream", title: "SoundStream: An End-to-End Neural Audio Codec", authors: "Zeghidour et al.", venue: "arXiv", year: 2021, arxivId: "2107.03312", tags: ["codec", "Google"], abstract: "First end-to-end neural audio codec.", citations: 500, era: "Generative", tier: "4" },
    { id: "vits", title: "VITS: Conditional Variational Autoencoder", authors: "Kim et al.", venue: "ICML", year: 2021, arxivId: "2106.06103", tags: ["TTS", "E2E", "landmark"], abstract: "End-to-end TTS with adversarial learning. High quality and speed.", citations: 1200, era: "Generative", tier: "4" },
    { id: "valle", title: "VALL-E: Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers", authors: "Microsoft", venue: "arXiv", year: 2023, arxivId: "2301.02111", tags: ["TTS", "zero-shot", "codec-lm"], abstract: "Treats TTS as a language modeling task on discrete audio codes.", citations: 1500, era: "Generative", tier: "4", mustRead: true },
    { id: "audiopalm", title: "AudioPaLM", authors: "Google", venue: "arXiv", year: 2023, arxivId: "2306.12925", tags: ["speech-to-speech", "multimodal"], abstract: "Unified model for speech understanding and generation.", citations: 234, era: "Generative", tier: "4" },
    { id: "cosyvoice", title: "CosyVoice: A Scalable Multilingual Voice Generation Model", authors: "Alibaba", venue: "arXiv", year: 2024, arxivId: "2407.05407", tags: ["TTS", "multilingual", "cloning"], abstract: "High-quality zero-shot voice cloning.", citations: 50, era: "Generative", tier: "4" },
    { id: "spiritlm", title: "Spirit LM: Interleaved Spoken and Written Language Model", authors: "Meta FAIR", venue: "arXiv", year: 2024, arxivId: "2402.05755", tags: ["speech-text", "emotion", "interleaved"], abstract: "First model to interleave speech and text tokens with emotional control.", citations: 120, era: "Generative", tier: "4", mustRead: true, oneLiner: "Speech/text interleaving enables emotional control in one unified model" },
    { id: "matcha-tts", title: "Matcha-TTS: A fast TTS architecture with conditional flow matching", authors: "Mehta et al.", venue: "ICASSP", year: 2024, arxivId: "2309.03199", tags: ["TTS", "flow-matching", "fast"], abstract: "Efficient TTS using optimal transport conditional flow matching.", citations: 85, era: "Generative", tier: "4" },
    { id: "soundstorm", title: "SoundStorm: Efficient Parallel Audio Generation", authors: "Google DeepMind", venue: "arXiv", year: 2023, arxivId: "2305.09636", tags: ["audio-generation", "parallel", "codec"], abstract: "Non-autoregressive audio generation with parallel decoding.", citations: 180, era: "Generative", tier: "4" },
    { id: "fastspeech2", title: "FastSpeech 2: Fast and High-Quality End-to-End TTS", authors: "Ren et al.", venue: "ICLR", year: 2020, arxivId: "2006.04558", tags: ["TTS", "non-autoregressive"], abstract: "Non-autoregressive TTS with pitch and duration prediction.", citations: 2500, era: "Deep Learning", tier: "4" },
    { id: "emotivoice", title: "EmotiVoice: A Multi-Voice and Multi-Emotion TTS", authors: "NetEase", venue: "arXiv", year: 2023, link: "https://github.com/netease-youdao/EmotiVoice", tags: ["TTS", "emotion", "open-source"], abstract: "Open-source TTS with emotion control for 2000+ speakers.", citations: 95, era: "Generative", tier: "4" },

    // ==========================================
    // Tier 5: Support (Benchmarks & Datasets)
    // ==========================================
    { id: "librispeech", title: "Librispeech: An ASR Corpus", authors: "Panayotov et al.", venue: "ICASSP", year: 2015, link: "https://www.openslr.org/12", tags: ["dataset", "ASR"], abstract: "1000 hours of public domain audiobooks. The standard ASR benchmark.", citations: 10000, era: "Deep Learning", tier: "5" },
    { id: "superb", title: "SUPERB: Speech Processing Universal PERformance Benchmark", authors: "Yang et al.", venue: "Interspeech", year: 2021, arxivId: "2105.01051", tags: ["benchmark", "ASR"], abstract: "Benchmark for universal speech representations.", citations: 900, era: "Generative", tier: "5" },
    { id: "mt-bench", title: "MT-Bench", authors: "LMSYS", venue: "arXiv", year: 2023, arxivId: "2306.05685", tags: ["benchmark", "LLM", "evaluation"], abstract: "Multi-turn question set for evaluating LLMs.", citations: 400, era: "Generative", tier: "5" },
    { id: "chatbot-arena", title: "Chatbot Arena", authors: "LMSYS", venue: "arXiv", year: 2024, arxivId: "2403.04132", tags: ["benchmark", "human-eval"], abstract: "Elo rating system based on pairwise human comparisons.", citations: 230, era: "Generative", tier: "5", mustRead: true, oneLiner: "Human preference is the ultimate metric for conversational AI" },
    { id: "vocalbench", title: "VocalBench", authors: "Various", venue: "arXiv", year: 2024, arxivId: "2024.vocalbench", tags: ["benchmark", "voice"], abstract: "Comprehensive benchmark for voice assistants.", citations: 30, era: "Generative", tier: "5" },
    { id: "full-duplex-bench", title: "Full-Duplex-Bench: A Benchmark to Evaluate Full-Duplex Models", authors: "Zhu et al.", venue: "arXiv", year: 2025, tags: ["benchmark", "full-duplex"], abstract: "Evaluating barge-in and turn-taking latency.", citations: 15, era: "Generative", tier: "5" },
    { id: "google-duplex", title: "Google Duplex", authors: "Google", venue: "Blog", year: 2018, tags: ["demo", "landmark"], abstract: "Famous haircutter booking demo showing natural phone conversations.", citations: 1250, era: "Deep Learning", tier: "5", mustRead: true, oneLiner: "First public demo proving AI can pass as human on the phone" },
    { id: "eval-critique", title: "How NOT to Evaluate Your Dialogue System", authors: "Liu et al.", venue: "EMNLP", year: 2016, arxivId: "1603.08023", tags: ["evaluation", "criticism"], abstract: "Showed word-overlap metrics don't correlate with human quality judgments.", citations: 1800, era: "Deep Learning", tier: "5" },
    { id: "voiceagenteval", title: "VoiceAgentEval: A Dual-Dimensional Benchmark for Expert-Level Intelligent Voice-Agent Evaluation", authors: "Xu et al. (Yao Guanghua)", venue: "arXiv", year: 2025, arxivId: "2510.21244", tags: ["benchmark", "evaluation", "voice-agent", "outbound"], abstract: "Comprehensive benchmark for evaluating LLMs in expert-level intelligent outbound calling scenarios. Covers 6 business domains and 30 sub-scenarios with User Simulator.", citations: 10, era: "Generative", tier: "5", mustRead: true, oneLiner: "Industry benchmark for voice agents in professional outbound scenarios" },

    // ==========================================
    // Tier 6: Global & Multilingual
    // ==========================================
    { id: "seamlessm4t", title: "SeamlessM4T: Massively Multilingual & Multimodal Machine Translation", authors: "Meta", venue: "arXiv", year: 2023, arxivId: "2308.11596", tags: ["translation", "multilingual", "multimodal"], abstract: "Unified model supporting 100+ languages in speech-to-speech, speech-to-text, text-to-speech translation.", citations: 450, highlight: true, era: "Generative", tier: "6", mustRead: true, oneLiner: "One model for all language pairs breaks the tower of Babel" },
    { id: "s2st-omni", title: "S2ST-Omni: An Efficient Multilingual Speech-to-Speech Translation", authors: "Yao et al.", venue: "arXiv", year: 2025, tags: ["S2ST", "omni", "multilingual"], abstract: "Efficient end-to-end speech translation without text intermediates.", citations: 25, era: "Generative", tier: "6" },
    { id: "f5-tts-crosslingual", title: "Cross-Lingual F5-TTS: Towards Language-Agnostic Voice Cloning", authors: "Chen et al.", venue: "arXiv", year: 2025, tags: ["TTS", "cloning", "cross-lingual"], abstract: "Zero-shot voice cloning that preserves speaker identity across languages.", citations: 18, era: "Generative", tier: "6" },
    { id: "cvss", title: "CVSS: A Massively Multilingual Speech-to-Speech Translation Corpus", authors: "Google", venue: "LREC", year: 2022, arxivId: "2201.03713", tags: ["corpus", "S2ST", "multilingual"], abstract: "Large-scale corpus for speech-to-speech translation research.", citations: 120, era: "Generative", tier: "6" },
];

